# 선형회귀(Linear Regression)
예측 값을 **직선**으로 표현하는 모델

![](https://user-images.githubusercontent.com/64197543/153241582-a73fc30d-8841-4be5-96ba-01e166f0ebfd.PNG)

![](https://user-images.githubusercontent.com/64197543/153241769-b8bbce6c-d0e3-4993-be9d-62b844237156.PNG)
데이터가 입력값이다.   


베타제로 : 편향
- 그래프에서 회귀선의 위치
- x가 0일 때의 예측 값
![](https://user-images.githubusercontent.com/64197543/153242354-990524bf-e670-47f3-80f9-9f99e90fd498.PNG)
![](https://user-images.githubusercontent.com/64197543/153242358-b94e09d6-84ba-4bf9-930b-ad542ae2c523.PNG)


베타원 : 계수(Coefficient)
- 회귀선의 기울기
- x가 1 증가할 때 예측 값이 증가하는 정도



![1](https://user-images.githubusercontent.com/64197543/153243075-6471c4ba-94a4-4a46-9db2-75952df8b61a.PNG)
실제 값과 예측 값의 차이가 작은 직선이 더 잘 예측한다.


<계산>   
실제 값의 부호보다는 실제값과 차이의 **크기**만 보고싶다!   
-> 제곱하고 평균을 구한다    

![2](https://user-images.githubusercontent.com/64197543/153243402-23936a20-827b-41d0-9420-3f1cdd298704.PNG)
![3](https://user-images.githubusercontent.com/64197543/153243385-d247ae06-775c-4f20-b6f7-aac112ab0d45.PNG)
![4](https://user-images.githubusercontent.com/64197543/153243396-95e7a066-3ac6-41f5-8c84-2027c0ace37d.PNG)
![5](https://user-images.githubusercontent.com/64197543/153243400-0fb4ae12-d90b-4d9a-a958-5e989135bf5f.PNG)

-> 차이가 더 적은 y = 1 + (1/2)x 가 더 잘 예측한다.


## 다변량 회귀 (Multivariate Regression)
두 개 이상의 변수로 만든 회귀식

**다변량 회귀 수식**
- 변수가 2개 인 경우
    y = B0 + B1x1 + B2x2    (B는 베타임)

- 변수가 여러 개인 경우
    -> 아래 사진처럼 행렬로 표시
![](https://user-images.githubusercontent.com/64197543/153245464-5050a34c-77c6-4b8a-afb2-42e2872c3856.PNG)
-> 1이 필요한 이유  :  편향인 베타 제로를 같이 곱했을 때 살리기 위해서

해석   
![](https://user-images.githubusercontent.com/64197543/153245710-d9f32a0a-849e-43c4-bcbb-df4bd4a15069.PNG)



## 다항식 회귀 (Polynomial Regression)
예측하는 값이 선형이 아닌 비선형일 경우 사용
 y = B0 + B1x + B2x^2
 
![1](https://user-images.githubusercontent.com/64197543/153246108-fb7f15c7-a2b0-401e-bf67-55d2d833f838.PNG)

**다항식 회귀와 다변량 회귀의 다른점**

![1](https://user-images.githubusercontent.com/64197543/153246264-41f346e2-bd11-4737-8d7a-5d449c636bba.PNG)

#### 비선형을 표현하는 법
1. 비선형 회귀식을 완전 제곱식으로 표현
2. x가 0 이상인 경우 비선형이 된다.
![1](https://user-images.githubusercontent.com/64197543/153247290-4aa2e9ff-d761-4359-b735-4ee4e4d7a51d.PNG)


#### 회귀 계수를 계산하는 방법
1. 통계적 방법
    - 최소 제곱법 : 이를 통해 구한 회귀값은 MSE가 최소가 된다.

![2](https://user-images.githubusercontent.com/64197543/153247292-ba9214ed-bcd4-4f3b-914d-933ca4663ce9.PNG)


2. ML 방법
여러 값을 넣어 본 뒤 loss(MSE)가 제일 작은 베타를 찾는다.    
-> 어떻게 하면 효율적으로 여러 값을 넣을 수 있을지    
-> 최적화 알고리즘    


#### 최적화 알고리즘
- Bisection Method 방법
1. 임의의 두개의 값을 설정한다
2. 두 값의 y 값을 비교한다
3. y값이 큰 점을 두 점의 가운데 점으로 바꾼다
4. 임의의 두 값의 차이가 작아질 때 까지 1~3을 반복한다



- Gradient Descent 방법 (경사 하강법)
함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것
1. 임의의 값 하나를 설정한다
2. 임의의 값에서의 기울기를 계산한다
3. 기울기와 Learning rate를 곱한 값을 임의의 값에서 뺀다
4. 기울기가 0에 가까워질 때 까지 1~3을 반복한다
